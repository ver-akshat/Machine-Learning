{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here I am implementing Adaboost classifier from scratch. Its also an ensemble technique like: Random Forest but it makes use of Boosting technique unless the bagging technique used in Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining helper functions like: compute error, alpha and updating weights.\n",
    "def total_error(y,y_pred,wi):\n",
    "    # calculating the total error which is used to calculate error for weak classifier and used in further steps.\n",
    "    return (sum(wi*(np.not_equal(y,y_pred)).astype(int)))/sum(wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(error):\n",
    "    # Calculate the weight of a weak classifier in the majority vote of the final classifier.it is also the importance of the feature.\n",
    "    return np.log((1-error)/error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(wi,alpha,y,y_pred):\n",
    "    # Update individual weights w_i after a boosting iteration\n",
    "    return wi*np.exp(alpha*(np.not_equal(y,y_pred)).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a class AdaBoost which define structure of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self):\n",
    "        self.alphas = []\n",
    "        self.D_T = []\n",
    "        self.n = None\n",
    "        self.training_errors = []\n",
    "        self.prediction_errors = []\n",
    "    \n",
    "    # defining the fit method:\n",
    "    def fit(self,X,y,n=100):\n",
    "        # here X=independent variable,y=target,n=no. of boosting rounds,defaule=100\n",
    "        self.alphas=[] \n",
    "        self.training_errors=[]\n",
    "        self.n=n\n",
    "\n",
    "        for i in range(0,n):\n",
    "            # for current boosting iteration set weights\n",
    "            if i==0:\n",
    "                wi=np.ones(len(y))*1/len(y) #bcoz at n=0 all weights are at same value\n",
    "            else:\n",
    "                wi=update_weights(wi,alpha_n,y,y_pred)\n",
    "            \n",
    "            # fit the weak classifier & make predictions:\n",
    "            # using a stump which is of depth 1\n",
    "            dt=DecisionTreeClassifier(max_depth=1)\n",
    "            dt.fit(X,y,sample_weight=wi)\n",
    "            y_pred=dt.predict(X)\n",
    "            self.D_T.append(dt)\n",
    "\n",
    "            # calculating error\n",
    "            error_n=total_error(y,y_pred,wi)\n",
    "            self.training_errors.append(error_n)\n",
    "\n",
    "            # calculating alpha\n",
    "            alpha_n=alpha(error_n)\n",
    "            self.alphas.append(alpha_n)\n",
    "\n",
    "    # defining predict method:\n",
    "    def predict(self,X):\n",
    "        weak_preds=pd.DataFrame(index=range(len(X)),columns=range(self.n))\n",
    "        # making a prediction for class label for weak classifier:\n",
    "        for i in range(self.n):\n",
    "            y_pred_n=self.D_T[i].predict(X)*self.alphas[i]\n",
    "            weak_preds.iloc[:,i]=y_pred_n\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "        return y_pred\n",
    "    \n",
    "    # Get the error rates of each weak classifier\n",
    "    def error_rates(self, X, y):\n",
    "        self.prediction_errors=[]\n",
    "        # Predict class label for each weak classifier\n",
    "        for i in range(self.n):\n",
    "            y_pred_n = self.D_T[i].predict(X)          \n",
    "            error_n =total_error(y=y,y_pred=y_pred_n,wi=np.ones(len(y)))\n",
    "            self.prediction_errors.append(error_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the class on some dataset and see the performance.Also I am going to compare the performance of my implemented class with inbuilt sklean AdaBoost Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again some imports \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a synthetic dataset\n",
    "X,y=make_classification(n_samples=800,n_features=10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y has only 0 and 1 but Adaboost uses -1 and 1 so converting 0 and 1 to -1 and 1 by:\n",
    "y=y*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into train and test set\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC-AUC score is :  0.951\n"
     ]
    }
   ],
   "source": [
    "adb=AdaBoost()\n",
    "adb.fit(X_train,y_train,n=100)\n",
    "# making predictions:\n",
    "y_pred=adb.predict(X_test)\n",
    "# calculating auc_roc\n",
    "print('The ROC-AUC score is : ',round(roc_auc_score(y_test,y_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC-AUC score of the model is: 0.932\n"
     ]
    }
   ],
   "source": [
    "# Comparing with sklearn implementation:\n",
    "adb_sklearn = AdaBoostClassifier(n_estimators=100)\n",
    "adb_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = adb_sklearn.predict(X_test)\n",
    "print('The ROC-AUC score of the model is:', round(roc_auc_score(y_test, y_pred_sklearn), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The class implemented customly and the sklearn inbuilt adaboost have comparable auc_roc score.Also sklearn use some different method for implementing AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7773864c198c8559e499b8a6d42753464881661d1a635729c4702e1dcc7c46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
